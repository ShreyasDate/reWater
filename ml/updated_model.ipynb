{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1 — Install (only if needed) and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "RND = 42\n",
    "np.random.seed(RND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 — Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"synthetic_wastewater_3years.csv\", parse_dates=[\"timestamp\"])\n",
    "print(\"Rows, cols:\", df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2.5 — Define reusability classes based on standard water quality guidelines\n",
    "\n",
    "  Based on WHO/EPA guidelines for water reuse (adapted for multiclass):\n",
    "- Drinking: Strictest (BOD<5, COD<10, TSS<1, pH 6.5-8.5, TDS<500)\n",
    "- Irrigation: Relaxed (BOD<30, COD<100, TSS<50, pH 6-9, TDS<2000)\n",
    "- Industrial: Moderate (BOD<100, COD<250, TSS<50, pH 6-9, TDS<2000)\n",
    "- Not_reusable: Fails all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_reusability(row):\n",
    "    bod = row['effluent_BOD']\n",
    "    cod = row['effluent_COD']\n",
    "    tss = row['effluent_TSS']\n",
    "    ph = row['effluent_pH']\n",
    "    tds = row['influent_TDS']  # Use influent_TDS as proxy for effluent TDS\n",
    "\n",
    "    if bod < 5 and cod < 10 and tss < 1 and 6.5 <= ph <= 8.5 and tds < 500:\n",
    "        return 'drinking'\n",
    "    elif bod < 30 and cod < 100 and tss < 50 and 6 <= ph <= 9 and tds < 2000:\n",
    "        return 'irrigation'\n",
    "    elif bod < 100 and cod < 250 and tss < 50 and 6 <= ph <= 9 and tds < 2000:\n",
    "        return 'industrial'\n",
    "    else:\n",
    "        return 'not_reusable'\n",
    "\n",
    "df['reusability'] = df.apply(classify_reusability, axis=1)\n",
    "\n",
    "print(\"Reusability distribution:\")\n",
    "print(df['reusability'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Quick data inspection\n",
    "Look for missing values and basic stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.isna().sum()\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — Visual checks (a few plots)\n",
    "\n",
    "Plot distributions of key columns and reusability distribution.  \n",
    "Histogram of influent_BOD and effluent_BOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "sns.histplot(df['influent_BOD'], bins=50, kde=True, ax=ax[0]).set_title(\"influent_BOD\")\n",
    "sns.histplot(df['effluent_BOD'], bins=50, kde=True, ax=ax[1]).set_title(\"effluent_BOD\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reusability distribution\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "df['reusability'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Reusability Class Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — Basic cleaning function\n",
    "\n",
    "- Ensure numeric columns exist\n",
    "- Interpolate small gaps\n",
    "- Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(df):\n",
    "    df = df.copy()\n",
    "    numeric_cols = [\n",
    "        \"influent_BOD\",\"influent_COD\",\"influent_TSS\",\"influent_pH\",\"influent_TDS\",\n",
    "        \"flow_rate\",\"aeration_rate\",\"chemical_dose\",\"sludge_recycle_rate\",\"retention_time\",\"temperature\",\n",
    "        \"effluent_BOD\",\"effluent_COD\",\"effluent_TSS\",\"effluent_pH\"\n",
    "    ]\n",
    "    # Ensure columns exist (create NaNs if missing)\n",
    "    for c in numeric_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    df = df.drop_duplicates().sort_values('timestamp').reset_index(drop=True)\n",
    "    # Interpolate numeric columns for small gaps\n",
    "    df[numeric_cols] = df[numeric_cols].interpolate(limit_direction='both', axis=0)\n",
    "    return df\n",
    "\n",
    "df = basic_clean(df)\n",
    "df.isnull().sum().loc[lambda x: x>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — Feature engineering\n",
    "\n",
    "Create rolling means, lag features, normalized dosing, hour/day/month features (for seasonality if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer(df):\n",
    "    df = df.copy()\n",
    "    # rolling 24-hour mean for influent_BOD (assumes hourly index)\n",
    "    df['influent_BOD_roll24'] = df['influent_BOD'].rolling(window=24, min_periods=1).mean()\n",
    "    # normalized chemical dose per m3\n",
    "    df['dose_per_m3'] = df['chemical_dose'] / (df['flow_rate'].replace(0, np.nan))\n",
    "    df['dose_per_m3'] = df['dose_per_m3'].fillna(0)\n",
    "    # lag of effluent_BOD (previous hour)\n",
    "    df['effluent_BOD_lag1'] = df['effluent_BOD'].shift(1).fillna(method='bfill')\n",
    "    # time features (optional — not used as primary features but useful if needed)\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "    return df\n",
    "\n",
    "\n",
    "df = feature_engineer(df)\n",
    "df[['timestamp','influent_BOD','influent_BOD_roll24','dose_per_m3','effluent_BOD_lag1','reusability']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Prepare features & target\n",
    "\n",
    "Define features list consistent with production pipeline. We will *not* include timestamp itself as a model feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \"flow_rate\",\n",
    "    \"influent_BOD\",\n",
    "    \"influent_COD\",\n",
    "    \"influent_TSS\",\n",
    "    \"influent_pH\",\n",
    "    \"influent_TDS\",\n",
    "    \"aeration_rate\",\n",
    "    \"chemical_dose\",\n",
    "    \"sludge_recycle_rate\",\n",
    "    \"retention_time\",\n",
    "    \"temperature\",\n",
    "    \"influent_BOD_roll24\",\n",
    "    \"dose_per_m3\",\n",
    "    \"effluent_BOD_lag1\"\n",
    "]\n",
    "\n",
    "TARGET = \"reusability\"\n",
    "\n",
    "# Ensure all FEATURES exist\n",
    "for f in FEATURES:\n",
    "    if f not in df.columns:\n",
    "        df[f] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "# Drop rows where TARGET is missing (model needs target)\n",
    "df_model = df.dropna(subset=[TARGET]).reset_index(drop=True)\n",
    "print(\"Rows available for modeling:\", len(df_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8 — Train/test split (time-based)\n",
    "\n",
    "Use the first 80% as training and last 20% as test (avoid time leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(df_model) * 0.8)\n",
    "train_df = df_model.iloc[:split_idx].copy()\n",
    "test_df = df_model.iloc[split_idx:].copy()\n",
    "print(\"Train rows:\", len(train_df), \"Test rows:\", len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9 — Build imputers, scaler, and label encoder (fit on TRAIN only)\n",
    "\n",
    "We compute medians for imputation and fit StandardScaler on training features. Encode target labels.  \n",
    "Imputer (median) and scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "scaler = StandardScaler()\n",
    "label_encoder = LabelEncoder()\n",
    "X_train_raw = train_df[FEATURES]\n",
    "X_test_raw = test_df[FEATURES]\n",
    "\n",
    "# Fit imputer on train\n",
    "imputer.fit(X_train_raw)\n",
    "X_train_imp = imputer.transform(X_train_raw)\n",
    "X_test_imp = imputer.transform(X_test_raw)\n",
    "\n",
    "# Fit scaler on imputed train\n",
    "scaler.fit(X_train_imp)\n",
    "X_train = scaler.transform(X_train_imp)\n",
    "X_test = scaler.transform(X_test_imp)\n",
    "\n",
    "# Encode target\n",
    "y_train_raw = train_df[TARGET]\n",
    "y_test_raw = test_df[TARGET]\n",
    "label_encoder.fit(y_train_raw)\n",
    "y_train = label_encoder.transform(y_train_raw)\n",
    "y_test = label_encoder.transform(y_test_raw)\n",
    "\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)\n",
    "print(\"Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10 — Train XGBoost classifier with early stopping\n",
    "\n",
    "We use XGBoost classifier with eval set and early stopping. Adjust hyperparameters as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=RND,\n",
    "    n_jobs=-1,\n",
    "    objective='multi:softprob',\n",
    "    num_class=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "# Use DMatrix-based early stopping via sklearn API's eval_set parameter\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11 — Evaluate model\n",
    "\n",
    "Compute accuracy, classification report, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12 — Feature importance (XGBoost built-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.Series(model.feature_importances_, index=FEATURES).sort_values(ascending=False)\n",
    "plt.figure(figsize=(8,5))\n",
    "fi.plot(kind='bar')\n",
    "plt.title(\"XGBoost feature_importances_\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13 — SHAP explainability (global + one sample)\n",
    "\n",
    " Note: SHAP can be resource-intensive. Use a small background sample.  \n",
    " Create explainer (TreeExplainer for xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "\n",
    "# Use a small background dataset (sample from train)\n",
    "bg = X_train[np.random.choice(len(X_train), size=min(500, len(X_train)), replace=False)]\n",
    "\n",
    "# Compute SHAP values for a subset of test set to save time\n",
    "X_test_sample = X_test[:1000]\n",
    "shap_values = explainer(X_test_sample)\n",
    "\n",
    "# Global summary plot\n",
    "shap.summary_plot(shap_values, features=X_test_sample, feature_names=FEATURES, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14 — Save artifacts for production\n",
    "\n",
    "We save: model.joblib, imputer.joblib, scaler.joblib, features.joblib, label_encoder.joblib, metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACT_DIR = \"models_v1\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "joblib.dump(model, os.path.join(ARTIFACT_DIR, \"model.joblib\"))\n",
    "joblib.dump(imputer, os.path.join(ARTIFACT_DIR, \"imputer.joblib\"))\n",
    "joblib.dump(scaler, os.path.join(ARTIFACT_DIR, \"scaler.joblib\"))\n",
    "joblib.dump(FEATURES, os.path.join(ARTIFACT_DIR, \"features.joblib\"))\n",
    "joblib.dump(label_encoder, os.path.join(ARTIFACT_DIR, \"label_encoder.joblib\"))\n",
    "\n",
    "\n",
    "meta = {\n",
    "    \"accuracy\": float(accuracy),\n",
    "    \"train_rows\": int(len(train_df)),\n",
    "    \"test_rows\": int(len(test_df)),\n",
    "    \"classes\": list(label_encoder.classes_),\n",
    "    \"trained_at\": pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "joblib.dump(meta, os.path.join(ARTIFACT_DIR, \"meta.joblib\"))\n",
    "\n",
    "print(\"Saved artifacts to:\", ARTIFACT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15 — Quick function: load artifacts and predict on a pandas DataFrame\n",
    "\n",
    "This emulates what your API will do: load imputer & scaler, preprocess features (same order), and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_artifacts(artifact_dir):\n",
    "    model = joblib.load(os.path.join(artifact_dir, \"model.joblib\"))\n",
    "    imputer = joblib.load(os.path.join(artifact_dir, \"imputer.joblib\"))\n",
    "    scaler = joblib.load(os.path.join(artifact_dir, \"scaler.joblib\"))\n",
    "    features = joblib.load(os.path.join(artifact_dir, \"features.joblib\"))\n",
    "    label_encoder = joblib.load(os.path.join(artifact_dir, \"label_encoder.joblib\"))\n",
    "    return model, imputer, scaler, features, label_encoder\n",
    "\n",
    "def preprocess_df_for_model(df_in, imputer, scaler, features):\n",
    "    # expects feature-engineered df (with same columns). We'll create missing features if absent.\n",
    "    X = df_in[features].copy()\n",
    "    for c in features:\n",
    "        if c not in X.columns:\n",
    "            X[c] = np.nan\n",
    "    X_imp = imputer.transform(X)\n",
    "    X_scaled = scaler.transform(X_imp)\n",
    "    return X_scaled\n",
    "\n",
    "# Load artifacts\n",
    "model_l, imputer_l, scaler_l, features_l, le_l = load_artifacts(ARTIFACT_DIR)\n",
    "\n",
    "# Example: predict first 5 rows from test_df\n",
    "X_test_proc = preprocess_df_for_model(test_df, imputer_l, scaler_l, features_l)\n",
    "preds_encoded = model_l.predict(X_test_proc[:5])\n",
    "preds_labels = le_l.inverse_transform(preds_encoded)\n",
    "print(\"Preds:\", preds_labels)\n",
    "print(\"Actuals:\", test_df[TARGET].values[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16 — (Optional) Save a small sample CSV with predictions\n",
    "\n",
    "Useful for demo or Streamlit preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_df.copy().head(1000)\n",
    "out_proc = preprocess_df_for_model(out, imputer_l, scaler_l, features_l)\n",
    "out['reusability_pred'] = le_l.inverse_transform(model_l.predict(out_proc))\n",
    "out[['timestamp','effluent_BOD','effluent_COD','effluent_TSS','effluent_pH','reusability','reusability_pred']].head()\n",
    "out.to_csv(\"predictions_sample.csv\", index=False)\n",
    "print(\"Saved sample predictions to predictions_sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 17 — (Optional) FastAPI snippet to serve predictions\n",
    "\n",
    "Run separately as `src/api.py`. This is sample code to show how to load artifacts and serve endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_code = \"\"\"\n",
    "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "import pandas as pd, io, joblib, os\n",
    "from pipeline import feature_engineer  # if you put FE logic in pipeline\n",
    "app = FastAPI()\n",
    "ARTIFACT_DIR = \"models_v1\"\n",
    "model = joblib.load(os.path.join(ARTIFACT_DIR, \"model.joblib\"))\n",
    "imputer = joblib.load(os.path.join(ARTIFACT_DIR, \"imputer.joblib\"))\n",
    "scaler = joblib.load(os.path.join(ARTIFACT_DIR, \"scaler.joblib\"))\n",
    "features = joblib.load(os.path.join(ARTIFACT_DIR, \"features.joblib\"))\n",
    "label_encoder = joblib.load(os.path.join(ARTIFACT_DIR, \"label_encoder.joblib\"))\n",
    "\n",
    "@app.post(\"/predict_csv\")\n",
    "async def predict_csv(file: UploadFile = File(...)):\n",
    "    if not file.filename.endswith(\".csv\"):\n",
    "        raise HTTPException(status_code=400, detail=\"CSV only\")\n",
    "    data = await file.read()\n",
    "    df = pd.read_csv(io.BytesIO(data), parse_dates=['timestamp'])\n",
    "    df = feature_engineer(df)  # apply the same FE you used in notebook\n",
    "    X = df[features].copy()\n",
    "    for c in features:\n",
    "        if c not in X.columns:\n",
    "            X[c] = np.nan\n",
    "    X_imp = imputer.transform(X)\n",
    "    X_scaled = scaler.transform(X_imp)\n",
    "    preds_encoded = model.predict(X_scaled)\n",
    "    preds_labels = label_encoder.inverse_transform(preds_encoded)\n",
    "    df['reusability_pred'] = preds_labels\n",
    "    return {\"rows\": len(df), \"preview\": df.head(10).to_dict(orient='records')}\n",
    "\"\"\"\n",
    "\n",
    "print(api_code[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final notes & next steps\n",
    "\n",
    "- The model now predicts reusability classes: 'not_reusable', 'industrial', 'irrigation', 'drinking'.\n",
    "- Based on effluent quality parameters.\n",
    "- Run the notebook cells to train and generate the saved artifacts in `models_v1`.\n",
    "- Ask me to produce the API files or repo scaffolding next."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
